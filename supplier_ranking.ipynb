{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading CSV data using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"expanded_green_supplier_selection.csv\")\n",
    "data_array = df.drop(columns=[\"Supplier\"]).to_numpy()\n",
    "weights_green = np.array([0.10, 0.10, 0.07, 0.06, 0.03, 0.05, 0.10, 0.07, 0.06, 0.04, 0.05, 0.05, 0.07, 0.06, 0.04, 0.02, 0.03])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalising Data and cleaning it for TOPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize data using MinMaxScaler\n",
    "def normalize_data(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighed Data Calculator\n",
    "It multiplies the weights to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighed_data(weight, data):\n",
    "    weighed_normal = weights_green * data\n",
    "    return weighed_normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the Positive Ideal Solution (Greatest positive deviation)\n",
    "and Negative Ideal Solution (Greatest negative Deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ideals(weighted_normal, benefits):\n",
    "    positive_ideal = np.max(weighted_normal, axis=0)\n",
    "    negative_ideal = np.min(weighted_normal, axis=0)\n",
    "    for i, benefit in enumerate(benefits):\n",
    "        if not benefit:\n",
    "            positive_ideal[i], negative_ideal[i] = negative_ideal[i], positive_ideal[i]\n",
    "    return positive_ideal, negative_ideal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidean Distance Calculator (Deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(weighted_data, ideal):\n",
    "    return np.sqrt(((weighted_data - ideal) ** 2).sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative Closeness Evaluation for TOPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def relative_closeness(positive_dist, negative_dist):\n",
    "    return negative_dist / (negative_dist + positive_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOPSIS evaluator for suppliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def topsis(data, weights, benefit_criteria):\n",
    "    norm_data = normalize_data(data)\n",
    "    weighted_norm = weighed_data(weights, norm_data)\n",
    "    \n",
    "    positive_ideal, negative_ideal = ideals(weighted_norm, benefit_criteria)\n",
    "    \n",
    "    distances_to_positive = distance(weighted_norm, positive_ideal)\n",
    "    distances_to_negative = distance(weighted_norm, negative_ideal)\n",
    "    \n",
    "    closeness_scores = relative_closeness(distances_to_positive, distances_to_negative)\n",
    "    \n",
    "\n",
    "    rankings = np.argsort(closeness_scores)[::-1]  # Sort in descending order\n",
    "    \n",
    "    return closeness_scores, rankings, positive_ideal, negative_ideal, weighted_norm, distances_to_positive, distances_to_negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# benefit criteria\n",
    "benefit_criteria = [False, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True]\n",
    "\n",
    "\n",
    "closeness_scores, rankings, positive_ideal, negative_ideal, weighted_norm, distances_to_positive, distances_to_negative= topsis(data_array, weights_green, benefit_criteria)\n",
    "\n",
    "\n",
    "print(\"Closeness Scores:\", closeness_scores)\n",
    "print(\"Rankings (from best to worst):\", rankings + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plot with scale for comparing Rankings of Suppliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of positive and negative distances\n",
    "positive_distances = distance(weighted_norm, positive_ideal)\n",
    "negative_distances = distance(weighted_norm, negative_ideal)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(positive_distances, negative_distances, c=closeness_scores, cmap='viridis', s=100)\n",
    "for i, txt in enumerate(range(1, len(closeness_scores) + 1)):\n",
    "    plt.annotate(txt, (positive_distances[i], negative_distances[i]), fontsize=9)\n",
    "plt.xlabel(\"Distance to Positive Ideal\")\n",
    "plt.ylabel(\"Distance to Negative Ideal\")\n",
    "plt.title(\"Scatter Plot of Distances to Ideals\")\n",
    "plt.colorbar(label=\"Closeness Score\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of closeness score of Suppliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bar plot of closeness scores\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.bar(range(1, len(closeness_scores) + 1), closeness_scores, color='skyblue')\n",
    "plt.xlabel(\"Supplier\")\n",
    "plt.ylabel(\"Closeness Score\")\n",
    "plt.title(\"Closeness Scores of Suppliers\")\n",
    "plt.xticks(range(1, len(closeness_scores) + 1))  # Label suppliers from 1 upwards\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x = df.drop(columns=['Supplier', 'Green Image (1-10)'])\n",
    "feature_names = x.columns\n",
    "y = df['Green Image (1-10)']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balance Dataset by using SMOTE(Synthetic Minority Oversampling Technique)\n",
    "It samples and uses the minority data again so that it is not underrepresented and dataset is well balanced for model to learn upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(columns=['Supplier', 'Green Image (1-10)'])\n",
    "y = df['Green Image (1-10)']\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Original dataset size:\", df.shape)\n",
    "print(\"Resampled dataset size:\", X_resampled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Random Forest Classifier for prediction of Green Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(random_state=0)\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict([[70,8,7,9,5,200,9,7,8,9,7,8,120,9,8,6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix and metrics for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using KNN for prediction of Green Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(x_train, y_train)\n",
    "a = [[90,2,7,9,5,90,7,7,8,9,7,8,120,9,8,6]]\n",
    "\n",
    "x_test_df = pd.DataFrame(a, columns=feature_names)\n",
    "model.predict(x_test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix and metrics for KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = model.predict(x_test)\n",
    "# Print evaluation metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
